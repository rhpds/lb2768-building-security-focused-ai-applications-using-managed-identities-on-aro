{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Working with an LLM programmatically\n",
    "\n",
    "You have certainly interacted before with a Large Language Model (LLM) like ChatGPT. This is usually done through a UI or an application.\n",
    "\n",
    "In this Notebook, we are going to use Python to connect and query an LLM directly through its API. For this Lab are using **Azure OpenAI GPT-4**.(https://azure.microsoft.com/en-us/blog/introducing-gpt4-in-azure-openai-service/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Library imports\n",
    "\n",
    "First we will import the libraries we need, they are already installed on our workbench image so no need to run `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else:\n",
    "    print(\"No file .env found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) is a framework for developing applications powered by language models. It reduces the amount of boilerplate code required to interface properly with different LLMs.\n",
    "\n",
    "We will start by creating an **llm** instance, defined by the location where the LLM API can be queried and some parameters that will be applied to the model. For example, `max_new_tokens` will instruct the model to answer with a maximum of 512 tokens (words or parts of words). `temperature`, set really low here, will instruct the model to stay truth-grounded, and not try to be too \"creative\". After all, we're not trying to write a fancy poem here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "optional_params = {\n",
    "  \"top_p\": 0.92,\n",
    "  \"presence_penalty\":1.03\n",
    "}\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_DEPLOYMENT\"),\n",
    "    max_tokens=512,\n",
    "    temperature=0.01,\n",
    "    model_kwargs=optional_params,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We also need a **template** to be applied to every request we are sending to the model (the \"Prompt\").\n",
    "\n",
    "When querying a model, you almost never want to send directly what the user has typed. On top of this entry, you need to give proper instructions to the model so that it knows how to handle it: what and how to answer, what NOT to answer, the tone it must use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer \"I don't know\".\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchain allows us to now easily \"stitch\" those elements together and create a **conversation** object that we will use to query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = PROMPT | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Artificial Intelligence?\"\n",
    "\n",
    "conversation.invoke ({\"input\": query});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089476-bba0-4093-8be8-1469780afaba",
   "metadata": {},
   "source": [
    "You can come back to this notebook at section 3.7 for some optional exercises if you want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
